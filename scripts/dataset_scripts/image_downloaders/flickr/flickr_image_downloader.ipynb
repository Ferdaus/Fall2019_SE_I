{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mPhrdmWCxH8S",
    "outputId": "b0bac2cb-6d43-49b7-a58f-b1d763b71221"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# gdrive_home_dir = '/content/drive/My Drive/'\n",
    "\n",
    "\n",
    "# !ls '{gdrive_home_dir}'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfrwpa0AyLhY"
   },
   "outputs": [],
   "source": [
    "# !pip install flickrapi\n",
    "\n",
    "# import flickrapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI0vkmzIyNge"
   },
   "outputs": [],
   "source": [
    "# Photo Source URL description: https://www.flickr.com/services/api/misc.urls.html\n",
    "\n",
    "# Find the id of a group by group name: https://www.flickr.com/services/api/explore/flickr.urls.lookupGroup\n",
    "# Get photos from a group: https://www.flickr.com/services/api/explore/flickr.groups.pools.getPhotos\n",
    "\n",
    "# flickr photo search api: https://www.flickr.com/services/api/explore/flickr.photos.search\n",
    "\n",
    "# Flickr photo licenses explained: https://www.flickr.com/creativecommons/\n",
    "# Flick photo licenses: https://www.flickr.com/services/api/explore/flickr.photos.licenses.getInfo\n",
    "\n",
    "# sample download scripts: https://gist.github.com/zmwangx/b1c16b197b5416143c7a\n",
    "#                          https://www.programcreek.com/python/example/6468/flickrapi.FlickrAPI\n",
    "#% reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2210
    },
    "colab_type": "code",
    "id": "1g3e7KlMxH8p",
    "outputId": "aea265ff-6bb8-4b02-f53e-3f3e047b35d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available images: 103\n",
      "Total available pages: 2\n",
      "[(1390240042, 1558409890)]\n",
      "Downloading page: 1\n",
      "Obtained total 100 image information from 1 pages, total obtained:100\n",
      "Downloading page: 2\n",
      "Obtained total 1 image information from 2 pages, total obtained:101\n",
      "Attempting to download total 101 images\n",
      "Number of image downloaders: 1\n",
      "Elapsed time 17.756003856658936 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://stackoverflow.com/questions/1994037/flickr-api-returning-duplicate-photos \n",
    "\n",
    "It is possible to retrieve more than 4000 images from flickr; your query has to be paginated by (for example) temporal range \n",
    "such that the total number of images from that query is not more than 4000. You can also use other parameters such as bounding \n",
    "box to limit the total number of images in the response.\n",
    "\n",
    "For example, if you are searching with the tag 'dogs', this is what you can do ( binary search over time range):\n",
    "\n",
    "Specify a minimum date and a maximum date in the request url, such as Jan 1st, 1990 and Jan 1st 2015.\n",
    "Inspect the total number of images in the response. If it is more than 4000, then divide the temporal range into two and \n",
    "work on the first half until you get less than 4000 images from the query. Once you get that, request all the pages from \n",
    "that time range, and move on to the next interval and do the same until (a) Number of required images is met (b) searched\n",
    "all over the initial time interval.\n",
    "\n",
    "'''\n",
    "\n",
    "#%reload_ext downloader_wrapper\n",
    "\n",
    "from flickr_dloader_utils import *\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Recommended way for creating a multi-process program in python\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    # number of images to download\n",
    "    num_images_to_download = 50000\n",
    "\n",
    "    # Specify whether to download from a group or by tags\n",
    "    mode = 'tags' # possible values: 'group' or 'tags'\n",
    "\n",
    "        # name of the group (if downloading from a group, ignored otherwise)\n",
    "    group_name = 'portrait-gallery'\n",
    "    group_name = 'selfie-shots'\n",
    "\n",
    "    # tags to be used to searching (if downloading using search tags, ignored otherwise)\n",
    "    tags = 'wefie'\n",
    "\n",
    "    base_download_dir = '/disks/data/datasets/selfie_project_datasets/downloaded_images/flickr'\n",
    "    \n",
    "    # to try time intervals \n",
    "    SECONDS_IN_A_YEAR = 31536000    \n",
    "    \n",
    "    CURRENT_TIME = int(time.time())\n",
    "    \n",
    "    # maximum number of times to retry if an image info page results in error\n",
    "    MAX_RETRY_COUNT = 3\n",
    "\n",
    "    # maximum number of images to assign to an image downloader process\n",
    "    MAX_LOAD_PER_IMG_DLOADER = 200 \n",
    "    \n",
    "    # maximum number of images information to be contained in a query result page.\n",
    "    # NOTE: Flickr sends the same set of image information if used a number higher than 100.\n",
    "    MAX_IMG_PER_PAGE = 100    \n",
    "    \n",
    "    # hack since api downloads 100 less than the specified number of images\n",
    "    num_images_to_download +=100 \n",
    "    \n",
    "    # finds the number of pages needed to be downloaded\n",
    "    #num_pages = int(math.ceil(num_images_to_download/MAX_IMG_PER_PAGE))\n",
    "\n",
    "    #print(f'Number of pages needed for {num_images_to_download} images: {num_pages}')\n",
    "    \n",
    "    # base download directory\n",
    "    #download_dir = f'{gdrive_home_dir}/dataset/test_dataset'\n",
    "        \n",
    "            \n",
    "    if mode == 'tags':        \n",
    "        download_dir = f'{base_download_dir}/{tags}_tags'\n",
    "    else:\n",
    "        download_dir = f'{base_download_dir}/{group_name}_group'\n",
    "        \n",
    "    \n",
    "    # settings for downloading images from a group or by search keywords/tags\n",
    "    configs = { 'group': {\n",
    "                            'search_criteria' : group_name,\n",
    "                            'download_dir' : download_dir,    \n",
    "                            #'image_info_save_file' : 'query_results_bygroup__' + group_name,\n",
    "                            'url_downloader' : download_flickr_img_by_group, # image info downloader function for groups\n",
    "                        },\n",
    "               'tags': {\n",
    "                            'search_criteria' : tags,\n",
    "                            'download_dir' : download_dir,    \n",
    "                            #'image_info_save_file' : 'query_results_bytags__' + \".\".join(tags.split(\",\")),\n",
    "                            'url_downloader' : download_flickr_img_url_by_tag, # image info downloader function for tags\n",
    "                        }\n",
    "              }\n",
    "\n",
    "    \n",
    "    if mode == 'tags':\n",
    "        \n",
    "        retry_count = 0\n",
    "        is_page_error = True\n",
    "        # find the number of images available for these tags\n",
    "        while retry_count < MAX_RETRY_COUNT and is_page_error:  \n",
    "            try:  \n",
    "\n",
    "                result = configs[mode]['url_downloader'](configs[mode]['search_criteria'])\n",
    "                is_page_error = False \n",
    "            except Exception as e:\n",
    "                # enforced delay not to overwhelm the Flickr API  \n",
    "                retry_count += 1\n",
    "                print('Retry attempt: ', retry_count)\n",
    "                time.sleep(2)    \n",
    "\n",
    "\n",
    "        # Number of pages in the result    \n",
    "        pagecount = result['photos']['pages']\n",
    "        imgCount = int(result['photos']['total'])\n",
    "\n",
    "        print(f\"Total available images: {imgCount}\")\n",
    "        print(f'Total available pages: {pagecount}')\n",
    "\n",
    "        results = []\n",
    "        search_time_intervals = []\n",
    "\n",
    "        # split the date range so that each query results in less than total 4000 images; otherwise flickr returns duplicates\n",
    "        if(imgCount > 4000):\n",
    "            \n",
    "            print('More than 4k images in query; segmenting the query')\n",
    "            photo_uploaddates = [photo['dateupload'] for photo in result['photos']['photo'] ]\n",
    "            photo_uploaddates.sort()\n",
    "            start_upload_date = int(photo_uploaddates[0])\n",
    "            #end_upload_date = start_upload_date + SECONDS_IN_A_YEAR\n",
    "\n",
    "            #print(first_photo_upload_date)                  \n",
    "            #print(result)\n",
    "\n",
    "            #unique_query_photos = { photo['id']:photo for result in results for photo in result['photos']['photo']}\n",
    "            #unique_query_photos = {}\n",
    "            total_img_count = 0\n",
    "\n",
    "            while( total_img_count < num_images_to_download and start_upload_date < CURRENT_TIME ):\n",
    "                interval_length = (CURRENT_TIME - start_upload_date)//2  #10*SECONDS_IN_A_YEAR\n",
    "                end_upload_date = start_upload_date + interval_length\n",
    "\n",
    "                start_time_h = datetime.datetime.utcfromtimestamp(start_upload_date).strftime('%Y-%m-%d %H:%M:%SZ')\n",
    "                end_time_h = datetime.datetime.utcfromtimestamp(end_upload_date).strftime('%Y-%m-%d %H:%M:%SZ')\n",
    "\n",
    "\n",
    "                retry_count = 0\n",
    "                is_page_error = True\n",
    "                while retry_count < MAX_RETRY_COUNT and is_page_error:  \n",
    "                    try:  \n",
    "                        # execute the query for finding out the number of pages available \n",
    "                        result = configs[mode]['url_downloader'](configs[mode]['search_criteria'], pagenum = None, \n",
    "                                                                   show_log = False, start_date = start_upload_date, end_date = end_upload_date)\n",
    "\n",
    "                        is_page_error = False \n",
    "                    except Exception as e:\n",
    "\n",
    "                        # enforced delay not to overwhelm the Flickr API  \n",
    "                        retry_count += 1\n",
    "                        print('Retry attempt: ', retry_count)\n",
    "                        time.sleep(1)    \n",
    "\n",
    "                time.sleep(0.5)\n",
    "                imgCount = int(result['photos']['total'])\n",
    "                print(\"image count: \", imgCount, \" (between \", end=\"\") \n",
    "                print(\"start: \", start_time_h, \" end:\", end_time_h, \")\" )\n",
    "\n",
    "                while(imgCount > 4000 and interval_length > 1):\n",
    "\n",
    "                    interval_length //= 2\n",
    "                    end_upload_date = start_upload_date + interval_length\n",
    "\n",
    "                    start_time_h = datetime.datetime.utcfromtimestamp(start_upload_date).strftime('%Y-%m-%d %H:%M:%SZ')\n",
    "                    end_time_h = datetime.datetime.utcfromtimestamp(end_upload_date).strftime('%Y-%m-%d %H:%M:%SZ')\n",
    "\n",
    "                    retry_count = 0\n",
    "                    is_page_error = True\n",
    "                    while retry_count < MAX_RETRY_COUNT and is_page_error:  \n",
    "                        try:  \n",
    "                            # execute the query for finding out the number of pages available \n",
    "                            result = configs[mode]['url_downloader'](configs[mode]['search_criteria'], pagenum = None, \n",
    "                                                                                                   show_log = False, \n",
    "                                        start_date = start_upload_date, end_date = end_upload_date)\n",
    "\n",
    "                            is_page_error = False \n",
    "                        except Exception as e:\n",
    "                            # enforced delay not to overwhelm the Flickr API  \n",
    "                            retry_count += 1\n",
    "                            print('Retry attempt: ', retry_count)\n",
    "                            time.sleep(2)                \n",
    "\n",
    "                    time.sleep(0.5)\n",
    "                    imgCount = int(result['photos']['total'])\n",
    "                    print(\"Search... image count: \", imgCount, \" (between \", end=\"\") \n",
    "                    print(\"start: \", start_time_h, \" end:\", end_time_h, \")\" )\n",
    "\n",
    "                # found query with less than 4k images, so add the start and end times to the list \n",
    "                search_time_intervals.append( (start_upload_date, end_upload_date ) )\n",
    "                total_img_count += imgCount\n",
    "\n",
    "                # update start time for the next query\n",
    "                start_upload_date = end_upload_date + 1\n",
    "\n",
    "                print(\"Total image count is: \", total_img_count)\n",
    "                print()\n",
    "\n",
    "        else:\n",
    "            results.append(result)\n",
    "            photo_uploaddates = [photo['dateupload'] for photo in result['photos']['photo'] ]\n",
    "            photo_uploaddates.sort()\n",
    "            start_upload_date = int(photo_uploaddates[0])\n",
    "            end_upload_date = CURRENT_TIME\n",
    "            search_time_intervals.append( (start_upload_date, end_upload_date ) )\n",
    "            \n",
    "        print(search_time_intervals)\n",
    "\n",
    "        query_photos = {}\n",
    "\n",
    "        for start_upload_date, end_upload_date in search_time_intervals:\n",
    "            retry_count = 0\n",
    "            is_page_error = True\n",
    "            while retry_count < MAX_RETRY_COUNT and is_page_error:  \n",
    "                try:  \n",
    "                    # execute the query for finding out the number of pages available \n",
    "                    # execute the query for finding out the number of pages available \n",
    "                    result = configs[mode]['url_downloader'](configs[mode]['search_criteria'], pagenum = None, \n",
    "                                                                                               show_log = False, \n",
    "                                    start_date = start_upload_date, end_date = end_upload_date)\n",
    "                    is_page_error = False \n",
    "                except Exception as e:\n",
    "                    # enforced delay not to overwhelm the Flickr API  \n",
    "                    retry_count += 1\n",
    "                    print('Retry attempt: ', retry_count)\n",
    "                    time.sleep(2)    \n",
    "\n",
    "\n",
    "                # Number of pages in the result    \n",
    "                pagecount = result['photos']['pages']\n",
    "\n",
    "                page_idx = 0\n",
    "\n",
    "\n",
    "                has_stalled = False\n",
    "\n",
    "                image_counts = []  \n",
    "                # download image info (can't be done parallely since can't instantiate more than one Flickr api instances with \n",
    "                # the same api key: possibly can be done by using one key per instance)          \n",
    "                while len(query_photos.items()) < num_images_to_download and page_idx < pagecount and not has_stalled:\n",
    "\n",
    "                    page_idx += 1\n",
    "\n",
    "                    is_page_error = True\n",
    "\n",
    "                    retry_count = 0\n",
    "\n",
    "                    while retry_count < MAX_RETRY_COUNT and is_page_error:  \n",
    "                        try:  \n",
    "                            # download image info as a dictionary and append to a list  \n",
    "                            result = configs[mode]['url_downloader'](configs[mode]['search_criteria'], page_idx, \n",
    "                                                                               show_log = True, \n",
    "                                    start_date = start_upload_date, end_date = end_upload_date)\n",
    "\n",
    "                            is_page_error = False \n",
    "                        except Exception as e:\n",
    "                            # enforced delay not to overwhelm the Flickr API  \n",
    "                            retry_count += 1\n",
    "                            print('Retry attempt: ', retry_count)\n",
    "                            time.sleep(2)\n",
    "\n",
    "                    results.append(result)\n",
    "                    # enforced delay not to overwhelm the Flickr API  \n",
    "                    time.sleep(0.5)\n",
    "\n",
    "                    # construct a dictionary of returned photos to eliminate ducplicate images (based on image id)      \n",
    "                    query_photos_this_page = { photo['id']:photo for photo in result['photos']['photo']}\n",
    "\n",
    "                    query_photos.update(query_photos_this_page)\n",
    "\n",
    "                    # compute the number of image information obtained       \n",
    "                    actual_total_image_count = len(query_photos.items()) # Flickr returns 100 less images for some reason\n",
    "\n",
    "                    image_counts.append(actual_total_image_count)\n",
    "\n",
    "                    print(f'Obtained total {len(query_photos_this_page.items())} image information from {page_idx} pages, total obtained:{actual_total_image_count}')\n",
    "\n",
    "                    if(len(image_counts) > 5 and image_counts[-1] < image_counts[-5] + 20):\n",
    "                        has_stalled = True\n",
    "\n",
    "        print(f'Attempting to download total {actual_total_image_count} images')          \n",
    "\n",
    "\n",
    "        ## Download the images from the downloaded image information           \n",
    "\n",
    "        # update num_process to reflect actual image count\n",
    "        num_processes = int(math.ceil(actual_total_image_count/MAX_LOAD_PER_IMG_DLOADER))\n",
    "\n",
    "        print(f'Number of image downloaders: {num_processes}')\n",
    "\n",
    "        # arrange the image downloader arguments for multi-process environment          \n",
    "        args = get_worker_args(num_processes, query_photos, MAX_LOAD_PER_IMG_DLOADER, configs[mode]['download_dir'], \n",
    "                                'c', None)\n",
    "\n",
    "        # for suppressing process output\n",
    "        def mute():\n",
    "            sys.stdout = open(os.devnull, 'w')   \n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        with Pool(num_processes) as p:\n",
    "            p.map(downloader_wrapper, args);\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        print(f'Elapsed time {end-start} seconds')\n",
    "    \n",
    "    else:   # for downloading from groups; date range not allowed in API call\n",
    "        \n",
    "        # finds the number of pages needed to be downloaded\n",
    "        num_pages = int(math.ceil(num_images_to_download/MAX_IMG_PER_PAGE))\n",
    "\n",
    "    \n",
    "        # execute the query for finding out the number of pages available \n",
    "        result = configs[mode]['url_downloader'](configs[mode]['search_criteria'])\n",
    "\n",
    "        # Number of pages in the result    \n",
    "        pagecount = result['photos']['pages']\n",
    "\n",
    "        print(f\"Total available images: {result['photos']['total']}\")\n",
    "        print(f'Total available pages: {pagecount}')\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # download image info (can't be done parallely since can't instantiate more than one Flickr api instances with \n",
    "        # the same api key)\n",
    "        for i in range( min(num_pages, pagecount) ):\n",
    "            \n",
    "            # download image info as a dictionary and append to a list  \n",
    "            results.append(configs[mode]['url_downloader'](configs[mode]['search_criteria'], i+1, show_log = True))\n",
    "\n",
    "            # enforced delay not to overwhelm the Flickr API  \n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # construct a dictionary of returned photos to eliminate ducplicate images (based on image id)      \n",
    "        query_photos = { photo['id']:photo for result in results for photo in result['photos']['photo']}\n",
    "\n",
    "        # compute the number of image information obtained       \n",
    "        actual_total_image_count = len(query_photos.items()) # Flickr returns 100 less images for some reason\n",
    "\n",
    "        print(f'Obtained total {actual_total_image_count} image information') \n",
    "              \n",
    "              \n",
    "        print(f'Attempting to download total {actual_total_image_count} images')          \n",
    "              \n",
    "        # update num_process to reflect actual image count\n",
    "        num_processes = int(math.ceil(actual_total_image_count/MAX_LOAD_PER_IMG_DLOADER))\n",
    "\n",
    "        print(f'Number of image downloaders: {num_processes}')\n",
    "              \n",
    "        # arrange the image downloader arguments for multi-process environment          \n",
    "        args = get_worker_args(num_processes, query_photos, MAX_LOAD_PER_IMG_DLOADER, \n",
    "                               configs[mode]['download_dir'], 'c', None)\n",
    "\n",
    "        # for suppressing process output\n",
    "        def mute():\n",
    "            sys.stdout = open(os.devnull, 'w')   \n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        with Pool(num_processes) as p:\n",
    "            p.map(downloader_wrapper, args);\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        print(f'Elapsed time {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2210
    },
    "colab_type": "code",
    "id": "1g3e7KlMxH8p",
    "outputId": "aea265ff-6bb8-4b02-f53e-3f3e047b35d9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRiX8-xUxH8x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dist_Flickr_image_downloaders.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
