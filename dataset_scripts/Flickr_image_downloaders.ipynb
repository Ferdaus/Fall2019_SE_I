{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photo Source URL description: https://www.flickr.com/services/api/misc.urls.html\n",
    "\n",
    "# Find the id of a group by group name: https://www.flickr.com/services/api/explore/flickr.urls.lookupGroup\n",
    "# Get photos from a group: https://www.flickr.com/services/api/explore/flickr.groups.pools.getPhotos\n",
    "\n",
    "# flickr photo search api: https://www.flickr.com/services/api/explore/flickr.photos.search\n",
    "\n",
    "# Flickr photo licenses explained: https://www.flickr.com/creativecommons/\n",
    "# Flick photo licenses: https://www.flickr.com/services/api/explore/flickr.photos.licenses.getInfo\n",
    "\n",
    "# sample download scripts: https://gist.github.com/zmwangx/b1c16b197b5416143c7a\n",
    "#                          https://www.programcreek.com/python/example/6468/flickrapi.FlickrAPI\n",
    "% reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from multiprocessing import Pool\n",
    "import flickrapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'c6a2c45591d4973ff525042472446ca2'\n",
    "secret = '202ffe6f387ce29b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flickr_search_base_config(pagenum):\n",
    "    \"\"\"\n",
    "    Generate base param configuration for Flickr API query.\n",
    "    \n",
    "    Params:\n",
    "        pagenum (int): Index (starting from 1) of the result page to fetch.  \n",
    "    \n",
    "    Returns: \n",
    "        A Python dictionary containing the configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    config = dict(per_page=100,\n",
    "                  content_type=1,\n",
    "                  license='1,2,3,4,5,6,9,10',\n",
    "                  extras='url_o,url_c',\n",
    "                  sort='relevance',\n",
    "                 )\n",
    "\n",
    "    # add pagenum to config if has valid value  \n",
    "    if not pagenum is None:\n",
    "        config['page'] = pagenum\n",
    "        \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_flickr_img(photos, download_dir='/disks/data/datasets/dloaded/', \n",
    "                        im_size = 'c', max_num = None, show_log = False, worker_id = None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Downloads images from Flickr.\n",
    "    \n",
    "    Params: \n",
    "        photos (sequence): Sequence of 'photo' elements (python dictionary) from Flickr API image search result.\n",
    "        download_dir (str): Location where the downloaded images will be stored. (current directory).\n",
    "        im_size (str): One of 'm', 'c', 'o', 'b' etc. See https://www.flickr.com/services/api/misc.urls.html for details.\n",
    "        max_num (int): Maximum number of images to download from the given sequence (None). \n",
    "        show_log (Bool): Enables logging (False).\n",
    "        worker_id (int): For tracking the worker this invokation belongs to (None).\n",
    "        \n",
    "    Returns: Does not return any value    \n",
    "    \"\"\"\n",
    "    \n",
    "    if show_log and not worker_id is None:\n",
    "        print(f\"image downloader#{worker_id} received list of {len(photos)} images \")\n",
    "        \n",
    "    # create the download directory if does not exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "        \n",
    "    # limit maximum number of images to be downloaded if was specified\n",
    "    if max_num is None:\n",
    "        pass\n",
    "    else:\n",
    "        photos = photos[:min(max_num, len(photos))]\n",
    "    \n",
    "    # build image URL from component information contained in the photo elements\n",
    "    def build_im_url(photo, im_size):\n",
    "        try:\n",
    "            farm_id = photo['farm']\n",
    "            server_id = photo['server']  \n",
    "            im_id = photo['id'] \n",
    "            secret = photo['secret'] \n",
    "            im_sz = im_size\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        # format: https://farm{farm-id}.staticflickr.com/{server-id}/{id}_{secret}_[mstzb].jpg\n",
    "        return f'https://farm{farm_id}.staticflickr.com/{server_id}/{im_id}_{secret}_{im_sz}.jpg'  \n",
    "\n",
    "    \n",
    "    for photo in photos:\n",
    "        #img_url = photo.get('url_c') or photo.get('url_o') or build_im_url(photo, im_size)\n",
    "        img_url = build_im_url(photo, im_size)\n",
    "    \n",
    "        # move on if image is not available\n",
    "        if img_url is None:\n",
    "          continue\n",
    "        \n",
    "        # build image name from location directory and image id\n",
    "        img_name = os.path.join(download_dir, img_url.split('/')[-1])\n",
    "        \n",
    "        # download and save image\n",
    "        urlretrieve(img_url, img_name)\n",
    "        \n",
    "        if show_log:\n",
    "            print('Downloaded '+img_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_flickr_img_url_by_tag(tags, tags_conjunct = True, search_title = True, pagenum = None, show_log = False):\n",
    "    \"\"\"\n",
    "    Downloads information about images matching the tag(s) \n",
    "    \n",
    "    Params:\n",
    "        tags (str): Comma separated tag words to search with\n",
    "        tags_conjunct (Bool): Indicates if intended tag combination is 'AND' (True).\n",
    "        search_title (Bool): Indicates if should search within the image title (True).\n",
    "        pagenum (int): Index (starting from 1) of the result page to fetch (None).                       \n",
    "        show_log (Bool): Enables logging (False).\n",
    "    \n",
    "    Returns: \n",
    "        dictionary containing the query result\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize flickr api\n",
    "    flickr = flickrapi.FlickrAPI(api_key, secret,format='parsed-json',\n",
    "                               store_token=False,\n",
    "                               cache=True)\n",
    "    \n",
    "    # generate base configuration for the flickr query    \n",
    "    config = get_flickr_search_base_config(pagenum)\n",
    "    \n",
    "    # add tag query specific params\n",
    "    if search_title:\n",
    "        config['text'] = \" \".join(tags.split(\",\"))\n",
    "        \n",
    "    config['tags'] = tags\n",
    "    config['tag_mode'] = 'all' if tags_conjunct else 'any' \n",
    "\n",
    "    # fetch image infos\n",
    "    result = flickr.photos.search(**config)\n",
    "    \n",
    "    if(show_log):\n",
    "        print(f\"Downloading page: {pagenum}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def download_flickr_img_by_group(group_name, pagenum = None, show_log = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Downloads information about images from a given Flickr group\n",
    "    \n",
    "    Params: \n",
    "        group_name (str): Name of the flickr group.\n",
    "        pagenum (int): Index (starting from 1) of the result page to fetch (None).                       \n",
    "        show_log (Bool): Enables logging (False).\n",
    "    \n",
    "    Returns: \n",
    "        dictionary containing the query result \n",
    "    \"\"\"\n",
    "\n",
    "    # initialize flickr api\n",
    "    flickr = flickrapi.FlickrAPI(api_key, secret,format='parsed-json', store_token=False, cache=True)\n",
    "    \n",
    "    # look up the id of the group by provided group name\n",
    "    groupinfo = flickr.urls.lookupGroup(url='https://www.flickr.com/groups/' + group_name + '/')\n",
    "    \n",
    "    # extract the group id from the result\n",
    "    group_id = groupinfo.get('group').get('id')\n",
    "        \n",
    "    # generate base configuration for the flickr query    \n",
    "    config = get_flickr_search_base_config(pagenum)\n",
    "\n",
    "    # add the group id to the config\n",
    "    config['group_id'] = group_id\n",
    "    \n",
    "    # retrieve information of the photos from the group\n",
    "    result = flickr.groups.pools.getPhotos(**config)\n",
    "\n",
    "    if(show_log):   \n",
    "        print(f\"Downloading page: {pagenum}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11546858/python-multiprocessing-keyword-arguments\n",
    "# https://stackoverflow.com/questions/34031681/passing-kwargs-with-multiprocessing-pool-map\n",
    "\n",
    "def downloader_wrapper(arg):\n",
    "    \"\"\"\n",
    "    Wrapper for the image downloader.\n",
    "    \n",
    "    Params:\n",
    "        arg (tuple): arguments (iterable, kwargs) to be passed to the image downloader.\n",
    "        \n",
    "    Returns: forwards image downloader result.     \n",
    "    \"\"\"\n",
    "    \n",
    "    args, kwargs = arg\n",
    "    \n",
    "    return download_flickr_img(args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worker_args(num_processes, query_photos_dict, max_img_dload_cnt_per_worker, download_dir, im_size, max_num):\n",
    "    \"\"\"\n",
    "        Creates a list of arguments for the image downloader processes\n",
    "        \n",
    "        Params:\n",
    "            num_processes (int): The intended number of image downloader processes\n",
    "            query_photos_dict (dictionary): Containing Flickr ids of the images as keys and 'photo' elements as values \n",
    "                               that contain information such as farm-id, secret, etc.\n",
    "            max_img_dload_cnt_per_worker (int): The maximum number of images to be assigned to a worker process.\n",
    "            download_dir (str): The base directory for saving the downloaded images. Each process has its own subdirectory\n",
    "                          under this dir in which it saves the images.\n",
    "            im_size (str): One of the available image sizes (e.g. 'c', 'm', 'b', 'o') in Flickr.\n",
    "            \n",
    "        Returns:\n",
    "            A list, each element of which corresponds to the argument to be passed to an image downloader process. \n",
    "    \"\"\"\n",
    "    \n",
    "    arg = []\n",
    "    \n",
    "    # Retrieve the photo elements as a list\n",
    "    query_photos = list(query_photos_dict.values())\n",
    "    \n",
    "    for i in range(num_processes):\n",
    "        # create the keyword arguments for the processes\n",
    "        kwargs = {   \n",
    "                     'download_dir' : os.path.join(download_dir,str(i+1)),\n",
    "                     'im_size' : im_size,\n",
    "                     'max_num' : max_num,\n",
    "                     'worker_id': i+1,\n",
    "                 }\n",
    "        \n",
    "        # compute the section end points' of the photo element list to be used for this worker process    \n",
    "        photo_start_idx = max_img_dload_cnt_per_worker*i\n",
    "        photo_end_idx = max_img_dload_cnt_per_worker*(i+1)\n",
    "        \n",
    "        # The regular argument and the keyword argument will be sent as argument to a process\n",
    "        arg.append((query_photos[photo_start_idx:photo_end_idx], kwargs))        \n",
    "        \n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image info file from disk for group selfie-shots\n",
      "Attempting to download total 9219 images\n",
      "Number of image downloaders: 47\n",
      "Elapsed time 295.4570646286011 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Recommended way for creating a multi-process program in python\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    # number of images to download\n",
    "    num_images_to_download = 11000 \n",
    "\n",
    "    # Specify whether to download from a group or by tags\n",
    "    config = 'group' # possible values: 'group' or 'tags'\n",
    "\n",
    "    # base download directory\n",
    "    download_dir = '/disks/data/datasets/dloaded/selfie-shots'\n",
    "\n",
    "\n",
    "    # name of the group (if downloading from a group, ignored otherwise)\n",
    "    group_name = 'selfie-shots'\n",
    "    # 'portrait-gallery'\n",
    "\n",
    "    # tags to be used to searching (if downloading using search tags, ignored otherwise)\n",
    "    tags = 'selfie'\n",
    "\n",
    "    \n",
    "    # maximum number of images to assign to an image downloader process\n",
    "    MAX_LOAD_PER_IMG_DLOADER = 200 \n",
    "    \n",
    "    # maximum number of images information to be contained in a query result page.\n",
    "    # NOTE: Flickr sends the same set of image information if used a number higher than 100.\n",
    "    MAX_IMG_PER_PAGE = 100    \n",
    "    \n",
    "    # hack since api downloads 100 less than the specified number of images\n",
    "    num_images_to_download +=100 \n",
    "    \n",
    "    # finds the number of pages needed to be downloaded\n",
    "    num_pages = int(math.ceil(num_images_to_download/MAX_IMG_PER_PAGE))\n",
    "\n",
    "    # settings for downloading images from a group or by search keywords\n",
    "    configs = { 'group': {\n",
    "                            'search_criteria' : group_name,\n",
    "                            'download_dir' : download_dir,    \n",
    "                            'image_info_save_file' : 'query_results_bygroup__' + group_name,\n",
    "                            'url_downloader' : download_flickr_img_by_group, # image info downloader function for groups\n",
    "                        },\n",
    "               'tags': {\n",
    "                            'search_criteria' : tags,\n",
    "                            'download_dir' : download_dir,    \n",
    "                            'image_info_save_file' : 'query_results_bygroup__' + \".\".join(tags.split(\",\")),\n",
    "                            'url_downloader' : download_flickr_img_url_by_tag, # image info downloader function for tags\n",
    "                        }\n",
    "              }\n",
    "    \n",
    "    # download image information if no saved version exists on disk\n",
    "    if not os.path.exists(configs[config]['image_info_save_file']):\n",
    "        \n",
    "        # execute the query for findinf out the number of pages available \n",
    "        result = configs[config]['url_downloader'](configs[config]['search_criteria'])\n",
    "\n",
    "        # Number of pages in the result    \n",
    "        pagecount = result['photos']['pages']\n",
    "\n",
    "        print(f\"Total available images: {result['photos']['total']}\")\n",
    "        print(f'Total available pages: {pagecount}')\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # download image info (can't be done parallely since can't instantiate more than one Flickr api instances with \n",
    "        # the same api key)\n",
    "        for i in range( min(num_pages, pagecount) ):\n",
    "            \n",
    "            # download image info as a dictionary and append to a list  \n",
    "            results.append(configs[config]['url_downloader'](configs[config]['search_criteria'], i, show_log = True))\n",
    "\n",
    "            # enforced delay not to overwhelm the Flickr API  \n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # construct a dictionary of returned photos to eliminate ducplicate images (based on image id)      \n",
    "        query_photos = { photo['id']:photo for result in results for photo in result['photos']['photo']}\n",
    "\n",
    "        # compute the number of image information obtained       \n",
    "        actual_total_image_count = len(query_photos.items()) # Flickr returns 100 less images for some reason\n",
    "\n",
    "        print(f'Obtained total {actual_total_image_count} image information')\n",
    "\n",
    "        # save the search result dictionary to disk to save time should we need to re-download      \n",
    "        pickle.dump(query_photos, open(configs[config]['image_info_save_file'],'wb'))\n",
    "\n",
    "    else:                  \n",
    "        \n",
    "        # load saved search result containing photo elements\n",
    "        query_photos = pickle.load(open(configs[config]['image_info_save_file'],'rb'))      \n",
    "    \n",
    "        print(f'Loaded image info file from disk for group {group_name}')\n",
    "              \n",
    "        # compute the number of image information obtained       \n",
    "        actual_total_image_count = len(query_photos.items()) # Flickr returns 100 less images for some reason\n",
    "        \n",
    "        if( num_images_to_download < actual_total_image_count):\n",
    "            im_ids = list(query_photos.keys())\n",
    "            im_ids = im_ids[:num_images_to_download]\n",
    "            photos = list(query_photos.values())\n",
    "            photos = photos[:num_images_to_download]\n",
    "              \n",
    "            query_photos = dict(zip(im_ids, photos))\n",
    "              \n",
    "            actual_total_image_count = num_images_to_download  \n",
    "    \n",
    "    print(f'Attempting to download total {actual_total_image_count} images')          \n",
    "              \n",
    "    # update num_process to reflect actual image count\n",
    "    num_processes = int(math.ceil(actual_total_image_count/MAX_LOAD_PER_IMG_DLOADER))\n",
    "              \n",
    "    print(f'Number of image downloaders: {num_processes}')\n",
    "              \n",
    "    # arrange the image downloader arguments for multi-process environment          \n",
    "    args = get_worker_args(num_processes, query_photos, MAX_LOAD_PER_IMG_DLOADER, configs[config]['download_dir'], \n",
    "                            'c', None)\n",
    "    \n",
    "    # for suppressing process output\n",
    "    def mute():\n",
    "        sys.stdout = open(os.devnull, 'w')   \n",
    "              \n",
    "    start = time.time()\n",
    "              \n",
    "    with Pool(num_processes) as p:\n",
    "        p.map(downloader_wrapper, args);\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Elapsed time {end-start} seconds')\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained total 1900 image information\n",
      "20\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not pagenum is None:\n",
    "#         download_flickr_img(result, download_dir, im_size, max_num, show_log)     \n",
    "#     else:\n",
    "# if not pagenum is None:\n",
    "#         download_flickr_img(result, download_dir, im_size, max_num, show_log)     \n",
    "#     else:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> calling `worker`\n",
      "Success\n",
      "=> no kwargs\n",
      "Fail\n",
      "Fail\n",
      "Fail\n",
      "Fail\n",
      "=> with `kwar_test=True`\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# import multiprocessing as mp\n",
    "\n",
    "# def worker_wrapper(arg):\n",
    "#     args, kwargs = arg\n",
    "#     return worker(*args, **kwargs)\n",
    "\n",
    "# def worker(x, y, **kwargs):\n",
    "#     kwarg_test = kwargs.get('kwarg_test', False)\n",
    "#     # print(\"kwarg_test = {}\".format(kwarg_test))     \n",
    "#     if kwarg_test:\n",
    "#         print(\"Success\")\n",
    "#     else:\n",
    "#         print(\"Fail\")\n",
    "#     return x*y\n",
    "\n",
    "# def wrapper_process(**kwargs):\n",
    "#     jobs = []\n",
    "#     pool=mp.Pool(4)\n",
    "#     for i, n in enumerate(range(4)):\n",
    "#         jobs.append((n,i))\n",
    "#     arg = [(j, kwargs) for j in jobs]\n",
    "#     pool.map(worker_wrapper, arg)\n",
    "\n",
    "# def main(**kwargs):\n",
    "#     print(\"=> calling `worker`\")\n",
    "#     worker(1, 2,kwarg_test=True) #accepts kwargs\n",
    "#     print(\"=> no kwargs\")\n",
    "#     wrapper_process() # no kwargs\n",
    "#     print(\"=> with `kwar_test=True`\")\n",
    "#     wrapper_process(kwarg_test=True)\n",
    "\n",
    "# if __name__ == \"__main__\":    \n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'slice' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-55ec84d375a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         image = image.resize((256, 256), Image.ANTIALIAS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         image.save('00001.jpg')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'slice' object is not iterable"
     ]
    }
   ],
   "source": [
    "# group_name = 'portrait-gallery'\n",
    "\n",
    "# pagecount = download_flickr_img_by_group(group_name,'portrait_gallery')['photos']['pages']\n",
    "\n",
    "# print(f'Total pages: {pagecount}')\n",
    "\n",
    "# print(config)\n",
    "# print(photo)\n",
    "# #print(result)\n",
    "\n",
    "#download_flickr_img(download_flickr_img_url_by_group('portrait-gallery'), 'portrait_group')\n",
    "#download_flickr_img(download_flickr_img_url_by_tag('portrait'), 'tag_portrait')\n",
    "\n",
    "\n",
    "#         image = Image.open(img_name) \n",
    "#         image = image.resize((256, 256), Image.ANTIALIAS)\n",
    "#         image.save('00001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Process\n",
    "# import os\n",
    "\n",
    "# def info(title):\n",
    "#     print(title)\n",
    "#     print('module name:', __name__)\n",
    "#     print('parent process:', os.getppid())\n",
    "#     print('process id:', os.getpid())\n",
    "\n",
    "# def f(name):\n",
    "#     info('function f')\n",
    "#     print('hello', name)\n",
    "\n",
    "# def foo(arg1,arg2):\n",
    "#     print(arg1, arg2)\n",
    "#     print()\n",
    "     \n",
    "# if __name__ == '__main__':\n",
    "#     #info('main line')\n",
    "#     p = Process(target=foo, args=[[1,2],[3,4]])\n",
    "#     p.start()\n",
    "#     p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "# from multiprocessing import Pool\n",
    "\n",
    "# def f(x):\n",
    "#     return x*x\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     with Pool(5) as p:\n",
    "#         print(p.map(f, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'photos': {'page': 2,\n",
    "#   'pages': 831,\n",
    "#   'perpage': 500,\n",
    "#   'total': '415358',\n",
    "#   'photo': [{'id': '45684177485',\n",
    "#     'owner': '55477049@N02',\n",
    "#     'secret': '378236928e',\n",
    "#     'server': '7802',\n",
    "#     'farm': 8,\n",
    "#     'title': 'Emma',\n",
    "#     'ispublic': 1,\n",
    "#     'isfriend': 0,\n",
    "#     'isfamily': 0,\n",
    "#     'ownername': 'henrychristo27 (Christophe)',\n",
    "#     'dateadded': '1546604057',\n",
    "#     'url_c': 'https://farm8.staticflickr.com/7802/45684177485_378236928e_c.jpg',\n",
    "#     'height_c': 534,\n",
    "#     'width_c': '800'},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json, sys\n",
    "# sys.path.append('../')\n",
    "# from personal import flikr_api_key as api_key\n",
    "\n",
    "\n",
    "# def get_requestURL(user_id,endpoint=\"getList\"):\n",
    "#     user_id = user_id.replace(\"@\",\"%40\")\n",
    "#     url_upto_apikey = (\"https://api.flickr.com/services/rest/?method=flickr.photosets.\" + \n",
    "#                        endpoint + \n",
    "#                        \"&api;_key=\" +  api_key + \n",
    "#                        \"&user;_id=\" +  user_id + \n",
    "#                        \"&format;=json&nojsoncallback;=1\")\n",
    "#     return(url_upto_apikey)\n",
    "\n",
    "# user_id = \"157237655@N08\"\n",
    "# url = get_requestURL(user_id,endpoint=\"getList\") \n",
    "# strlist = requests.get(url).content\n",
    "# json_data = json.loads(strlist)\n",
    "# albums = json_data[\"photosets\"][\"photoset\"]\n",
    "\n",
    "# print(\"{} albums found for user_id={}\".format(len(albums),user_id))\n",
    "\n",
    "# photosetids, titles = [], []\n",
    "# for album in albums:\n",
    "#     print(\"___\")\n",
    "#     print(\"album title={} photoset_id={}\".format(album['title']['_content'],album[\"id\"]))\n",
    "#     photosetids.append(album[\"id\"])\n",
    "#     titles.append(album['title']['_content'])\n",
    "    \n",
    "# def get_photo_url(farmId,serverId,Id,secret):\n",
    "#     return ((\"https://farm\" + str(farmId) + \n",
    "#             \".staticflickr.com/\" + serverId + \n",
    "#             \"/\" + Id + '_' + secret + \".jpg\"))\n",
    "\n",
    "# URLs = {}\n",
    "# for photoset_id, title in zip(photosetids,titles): ## for each album\n",
    "#     url = get_requestURL(user_id,endpoint=\"getPhotos\") + \"&photoset;_id=\" + photoset_id\n",
    "#     strlist = requests.get(url).content\n",
    "#     json1_data = json.loads(strlist)\n",
    "    \n",
    "#     urls = []\n",
    "#     for pic in json1_data[\"photoset\"][\"photo\"]: ## for each picture in an album\n",
    "#         urls.append(get_photo_url(pic[\"farm\"],pic['server'], pic[\"id\"], pic[\"secret\"]))\n",
    "        \n",
    "#     URLs[photoset_id] = urls\n",
    "    \n",
    "\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# count = 1\n",
    "# for i, (photoset_id, urls) in enumerate(URLs.items()):\n",
    "#     print(\"______________________\")\n",
    "#     print(\"{}, photoset_id={}\".format(titles[i],photoset_id))\n",
    "#     for url in urls:\n",
    "#         print(url)\n",
    "#         display(Image(url= url, width=200, height=200))\n",
    "#     count += 1\n",
    "#     if count > 4:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
